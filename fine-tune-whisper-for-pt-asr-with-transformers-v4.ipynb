{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b58048-7d14-4fc6-8085-1fc08c81b4a6",
   "metadata": {
    "id": "75b58048-7d14-4fc6-8085-1fc08c81b4a6"
   },
   "source": [
    "# Fine-Tune Whisper For Multilingual ASR with ü§ó Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa8ad5-4cdc-4512-9058-836cbbf65e1a",
   "metadata": {
    "id": "fbfa8ad5-4cdc-4512-9058-836cbbf65e1a"
   },
   "source": [
    "In this Colab, we present a step-by-step guide on how to fine-tune Whisper\n",
    "for any multilingual ASR dataset using Hugging Face ü§ó Transformers. This is a\n",
    "more \"hands-on\" version of the accompanying [blog post](https://huggingface.co/blog/fine-tune-whisper).\n",
    "For a more in-depth explanation of Whisper, the Common Voice dataset and the theory behind fine-tuning, the reader is advised to refer to the blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0d503-ae4e-4aa7-9af4-dbcba52db41e",
   "metadata": {
    "id": "afe0d503-ae4e-4aa7-9af4-dbcba52db41e"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb616d4-b610-4c90-bad9-60ebddc477d2",
   "metadata": {},
   "source": [
    "Whisper √© um modelo pr√©-treinado para reconhecimento autom√°tico de fala (ASR), publicado em [setembro de 2022](https://openai.com/blog/whisper/) pelos autores Alec Radford et al., da OpenAI. Diferente de muitos de seus predecessores, como [Wav2Vec 2.0](https://arxiv.org/abs/2006.11477), que s√£o pr√©-treinados em dados de √°udio n√£o rotulados, o Whisper √© pr√©-treinado em uma grande quantidade de dados de √°udio **rotulados**, totalizando 680.000 horas. Isso representa uma ordem de magnitude maior do que os 60.000 horas de dados de √°udio n√£o rotulados usados para treinar o Wav2Vec 2.0.  \n",
    "\n",
    "Al√©m disso, 117.000 horas desses dados de pr√©-treinamento correspondem a dados multil√≠ngues de ASR. Como resultado, os pontos de verifica√ß√£o do modelo podem ser aplicados a mais de 96 idiomas, muitos dos quais s√£o considerados _de poucos recursos_.  \n",
    "\n",
    "Quando escalados para 680.000 horas de dados de pr√©-treinamento rotulados, os modelos Whisper demonstram uma forte capacidade de generaliza√ß√£o para diversos conjuntos de dados e dom√≠nios. Os pontos de verifica√ß√£o pr√©-treinados alcan√ßam resultados competitivos em rela√ß√£o aos sistemas de ASR de √∫ltima gera√ß√£o, com uma taxa de erro de palavras (WER) pr√≥xima a 3% no subconjunto test-clean do LibriSpeech ASR, al√©m de estabelecer um novo recorde no TED-LIUM com 4,7% de WER (_ver_ Tabela 8 do [artigo do Whisper](https://cdn.openai.com/papers/whisper.pdf)).  \n",
    "\n",
    "O amplo conhecimento adquirido pelo Whisper durante o pr√©-treinamento para ASR multil√≠ngue pode ser aproveitado para outros idiomas de poucos recursos. Atrav√©s do fine-tuning, os pontos de verifica√ß√£o pr√©-treinados podem ser adaptados para conjuntos de dados e idiomas espec√≠ficos, aprimorando ainda mais esses resultados. Neste Colab, mostraremos como o Whisper pode ser ajustado para idiomas de poucos recursos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b91d6-be24-4b5e-bb38-4977ea143a72",
   "metadata": {
    "id": "e59b91d6-be24-4b5e-bb38-4977ea143a72"
   },
   "source": [
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/whisper_architecture.svg\" alt=\"Trulli\" style=\"width:100%\">\n",
    "<figcaption align=\"center\"><b>Figura 1:</b> Modelo Whisper. A arquitetura segue o modelo padr√£o de codificador-decodificador baseado em Transformer.  \n",
    "Um espectrograma log-Mel √© inserido no codificador. Os √∫ltimos estados ocultos do codificador s√£o passados para o decodificador por meio de mecanismos de aten√ß√£o cruzada.  \n",
    "O decodificador prev√™ tokens de texto de forma autoregressiva, condicionando-se conjuntamente aos estados ocultos do codificador e aos tokens previstos anteriormente.  \n",
    "Fonte da figura: <a href=\"https://openai.com/blog/whisper/\">Blog OpenAI Whisper</a>.  \n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6316e-8a55-4549-a154-66d3da2ab74a",
   "metadata": {
    "id": "21b6316e-8a55-4549-a154-66d3da2ab74a"
   },
   "source": [
    "Os pontos de verifica√ß√£o (checkpoints) do Whisper est√£o dispon√≠veis em cinco configura√ß√µes com tamanhos variados de modelo. Os quatro menores foram treinados em dados exclusivamente em ingl√™s ou em dados multil√≠ngues. J√° os maiores foram treinados apenas em dados multil√≠ngues. Todos os 11 pontos de verifica√ß√£o pr√©-treinados est√£o dispon√≠veis no [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). A tabela abaixo resume esses pontos de verifica√ß√£o, com links para os modelos no Hub:  \n",
    "\n",
    "| Tamanho  | Camadas | Largura | Cabe√ßas | Par√¢metros | Apenas em Ingl√™s                                   | Multil√≠ngue                                      |\n",
    "|----------|--------|---------|---------|------------|--------------------------------------------------|-------------------------------------------------|\n",
    "| tiny     | 4      | 384     | 6       | 39 M       | [‚úì](https://huggingface.co/openai/whisper-tiny.en)   | [‚úì](https://huggingface.co/openai/whisper-tiny.)    |\n",
    "| base     | 6      | 512     | 8       | 74 M       | [‚úì](https://huggingface.co/openai/whisper-base.en)   | [‚úì](https://huggingface.co/openai/whisper-base)     |\n",
    "| small    | 12     | 768     | 12      | 244 M      | [‚úì](https://huggingface.co/openai/whisper-small.en)  | [‚úì](https://huggingface.co/openai/whisper-small)    |\n",
    "| medium   | 24     | 1024    | 16      | 769 M      | [‚úì](https://huggingface.co/openai/whisper-medium.en) | [‚úì](https://huggingface.co/openai/whisper-medium)   |\n",
    "| large    | 32     | 1280    | 20      | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large)    |\n",
    "| large-v2 | 32     | 1280    | 20      | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large-v2) |\n",
    "| large-v3 | 32     | 1280    | 20      | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large-v3) |\n",
    "\n",
    "Para fins de demonstra√ß√£o, faremos o fine-tuning da vers√£o multil√≠ngue do checkpoint [`\"small\"`](https://huggingface.co/openai/whisper-small), que possui 244M de par√¢metros (~1GB).  \n",
    "\n",
    "Quanto aos dados, treinaremos e avaliaremos nosso sistema em um idioma de poucos recursos, retirado do conjunto de dados [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). Mostraremos que, com apenas 8 horas de dados de fine-tuning, √© poss√≠vel alcan√ßar um desempenho s√≥lido nesse idioma.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb341f59-fb9f-4214-a61f-97da758cd664",
   "metadata": {},
   "source": [
    "---\n",
    "\\\\({}^1\\\\) O nome Whisper deriva do acr√¥nimo ‚ÄúWSPSR‚Äù, que significa ‚ÄúWeb-scale Supervised Pre-training for Speech Recognition‚Äù (‚ÄúPr√©-treinamento Supervisionado em Escala Web para Reconhecimento de Fala‚Äù)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb8d21-df06-472a-99dd-b59567be6dad",
   "metadata": {
    "id": "55fb8d21-df06-472a-99dd-b59567be6dad"
   },
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a4861-929c-4762-b29b-80b1e95aba4b",
   "metadata": {
    "id": "844a4861-929c-4762-b29b-80b1e95aba4b"
   },
   "source": [
    "Primeiramente, vamos tentar garantir uma GPU decente para nosso Colab! Infelizmente, est√° cada vez mais dif√≠cil obter acesso a uma boa GPU na vers√£o gratuita do Google Colab. No entanto, com o Google Colab Pro, voc√™ n√£o deve ter problemas para ser alocado com uma GPU V100 ou P100.  \n",
    "\n",
    "Para obter uma GPU, clique em _Runtime_ -> _Change runtime type_, depois altere _Hardware accelerator_ de _CPU_ para uma das GPUs dispon√≠veis, como _T4_ (ou uma melhor, se houver). Em seguida, clique em `Connect T4` no canto superior direito da tela (ou `Connect {V100, A100}` caso tenha selecionado outra GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abea5d7-9d54-434b-a6bd-399d1b3c6c1a",
   "metadata": {
    "id": "9abea5d7-9d54-434b-a6bd-399d1b3c6c1a"
   },
   "source": [
    "Podemos verificar se fomos atribu√≠dos a uma GPU e visualizar suas especifica√ß√µes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95048026-a3b7-43f0-a274-1bad65e407b4",
   "metadata": {
    "id": "95048026-a3b7-43f0-a274-1bad65e407b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 22 11:44:30 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.256.02   Driver Version: 470.256.02   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| 30%   47C    P8    17W / 175W |    719MiB /  7948MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1173      G   /usr/lib/xorg/Xorg                101MiB |\n",
      "|    0   N/A  N/A      1679      G   /usr/lib/xorg/Xorg                224MiB |\n",
      "|    0   N/A  N/A      1810      G   /usr/bin/gnome-shell               97MiB |\n",
      "|    0   N/A  N/A      3369      G   ...lcad/anaconda3/bin/python        2MiB |\n",
      "|    0   N/A  N/A      3665      G   /usr/lib/firefox/firefox          278MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85d613-1c7e-46ac-9134-660bbe7ebc9d",
   "metadata": {
    "id": "1d85d613-1c7e-46ac-9134-660bbe7ebc9d"
   },
   "source": [
    "Usaremos v√°rios pacotes populares do Python para fazer o fine-tuning do modelo Whisper.  \n",
    "Usaremos o `datasets[audio]` para baixar e preparar nossos dados de treinamento, junto com o `transformers` e o `accelerate` para carregar e treinar nosso modelo Whisper.  \n",
    "Tamb√©m precisaremos do pacote `soundfile` para pr√©-processar arquivos de √°udio, `evaluate` e `jiwer` para avaliar o desempenho do nosso modelo, e `tensorboard` para registrar nossas m√©tricas. Por fim, usaremos o `gradio` para construir uma demonstra√ß√£o visual do nosso modelo ajustado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e68ea9f8-9b61-414e-8885-3033b67c2850",
   "metadata": {
    "id": "e68ea9f8-9b61-414e-8885-3033b67c2850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/lcad/anaconda3/lib/python3.12/site-packages (25.0.1)\n",
      "Requirement already satisfied: transformers in /home/lcad/anaconda3/lib/python3.12/site-packages (4.50.0)\n",
      "Requirement already satisfied: accelerate in /home/lcad/anaconda3/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: evaluate in /home/lcad/anaconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: jiwer in /home/lcad/anaconda3/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: tensorboard in /home/lcad/anaconda3/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: gradio in /home/lcad/anaconda3/lib/python3.12/site-packages (5.22.0)\n",
      "Requirement already satisfied: datasets[audio] in /home/lcad/anaconda3/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: filelock in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets[audio]) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (0.29.3)\n",
      "Requirement already satisfied: packaging in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (6.0.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (0.13.1)\n",
      "Requirement already satisfied: librosa in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (0.11.0)\n",
      "Requirement already satisfied: soxr>=0.4.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from datasets[audio]) (0.5.0.post1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/lcad/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/lcad/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/lcad/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: psutil in /home/lcad/anaconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/lcad/anaconda3/lib/python3.12/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/lcad/anaconda3/lib/python3.12/site-packages (from jiwer) (3.12.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/lcad/anaconda3/lib/python3.12/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/lcad/anaconda3/lib/python3.12/site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/lcad/anaconda3/lib/python3.12/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/lcad/anaconda3/lib/python3.12/site-packages (from tensorboard) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /home/lcad/anaconda3/lib/python3.12/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (0.115.11)\n",
      "Requirement already satisfied: ffmpy in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.8.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (1.8.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (3.10.15)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (2.8.2)\n",
      "Requirement already satisfied: pydub in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (0.11.0)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (0.46.1)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/lcad/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets[audio]) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/lcad/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets[audio]) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets[audio]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets[audio]) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/lcad/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets[audio]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets[audio]) (1.11.0)\n",
      "Requirement already satisfied: certifi in /home/lcad/anaconda3/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/lcad/anaconda3/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/lcad/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/lcad/anaconda3/lib/python3.12/site-packages (from pandas->datasets[audio]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from pandas->datasets[audio]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/lcad/anaconda3/lib/python3.12/site-packages (from pandas->datasets[audio]) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lcad/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets[audio]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets[audio]) (2.2.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from soundfile>=0.12.1->datasets[audio]) (1.17.1)\n",
      "Requirement already satisfied: networkx in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/lcad/anaconda3/lib/python3.12/site-packages (from librosa->datasets[audio]) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from librosa->datasets[audio]) (0.60.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from librosa->datasets[audio]) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from librosa->datasets[audio]) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from librosa->datasets[audio]) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from librosa->datasets[audio]) (5.1.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from librosa->datasets[audio]) (1.8.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from librosa->datasets[audio]) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from librosa->datasets[audio]) (1.0.3)\n",
      "Requirement already satisfied: pycparser in /home/lcad/anaconda3/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from pooch>=1.1->librosa->datasets[audio]) (3.10.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/lcad/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa->datasets[audio]) (3.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/lcad/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60d173-8de1-4ed7-bc9a-d281cf237203",
   "metadata": {
    "id": "1f60d173-8de1-4ed7-bc9a-d281cf237203"
   },
   "source": [
    "Recomendamos fortemente que voc√™ fa√ßa o upload dos pontos de verifica√ß√£o do modelo diretamente no [Hugging Face Hub](https://huggingface.co/) durante o treinamento. O Hub oferece:\n",
    "\n",
    "- Controle de vers√£o integrado: voc√™ pode ter certeza de que nenhum ponto de verifica√ß√£o do modelo ser√° perdido durante o treinamento.\n",
    "- Logs do Tensorboard: acompanhe m√©tricas importantes ao longo do treinamento.\n",
    "- Cart√µes de modelo: documente o que o modelo faz e seus casos de uso pretendidos.\n",
    "- Comunidade: uma maneira f√°cil de compartilhar e colaborar com a comunidade!\n",
    "\n",
    "Vincular o notebook ao Hub √© simples - basta inserir o token de autentica√ß√£o do Hub quando solicitado. Encontre seu token de autentica√ß√£o do Hub [aqui](https://huggingface.co/settings/tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63d42528-41a9-48d6-8f49-0986ca71ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b045a39e-2a3e-4153-bdb5-281500bcd348",
   "metadata": {
    "id": "b045a39e-2a3e-4153-bdb5-281500bcd348"
   },
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aea4401a-f95e-4995-b012-b7dc90e5c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Lendo a chave do arquivo token.txt\n",
    "with open(os.path.join(\"key.txt\"), \"r\") as file:\n",
    "    token = file.read().strip()  # .strip() remove espa√ßos extras e quebras de linha\n",
    "\n",
    "# Fazer login com a chave\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0",
   "metadata": {
    "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674429c5-0ab4-4adf-975b-621bb69eca38",
   "metadata": {
    "id": "674429c5-0ab4-4adf-975b-621bb69eca38"
   },
   "source": [
    "Usando ü§ó Datasets, baixar e preparar os dados √© extremamente simples. Podemos baixar e preparar as divis√µes do Common Voice em apenas uma linha de c√≥digo.\n",
    "\n",
    "Primeiro, certifique-se de ter aceitado os termos de uso no Hugging Face Hub: [mozilla-foundation/common_voice_17_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0). Depois de aceitar os termos, voc√™ ter√° acesso total ao conjunto de dados e poder√° fazer o download dos dados localmente.\n",
    "\n",
    "Como o Portugu√™s √© um idioma de baixo recurso, vamos combinar as divis√µes `train` e `validation` para obter aproximadamente 8 horas de dados de treinamento. Usaremos as 4 horas de dados de `test` como nosso conjunto de testes separado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
   "metadata": {
    "id": "a2787582-554f-44ce-9f38-4180a5ed6b44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 31432\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 9467\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"pt\", split=\"train+validation\", trust_remote_code=True)#use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"pt\", split=\"test\", trust_remote_code=True)#use_auth_token=True)\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c7c3d6-7197-41e7-a088-49b753c1681f",
   "metadata": {
    "id": "d5c7c3d6-7197-41e7-a088-49b753c1681f"
   },
   "source": [
    "A maioria dos conjuntos de dados de ASR fornece apenas amostras de √°udio de entrada (`√°udio`) e o texto transcrito correspondente (`senten√ßa`). O Common Voice cont√©m informa√ß√µes adicionais de metadados, como `sotaque` e `localidade`, que podemos desconsiderar para ASR. Mantendo o notebook o mais geral poss√≠vel, consideramos apenas o √°udio de entrada e o texto transcrito para o fine-tuning, descartando as informa√ß√µes adicionais de metadados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce",
   "metadata": {
    "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence', 'variant'],\n",
      "        num_rows: 31432\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence', 'variant'],\n",
      "        num_rows: 9467\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605",
   "metadata": {
    "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605"
   },
   "source": [
    "## Prepare Feature Extractor, Tokenizer and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c3099-1026-439e-93e2-5635b3ba5a73",
   "metadata": {
    "id": "601c3099-1026-439e-93e2-5635b3ba5a73"
   },
   "source": [
    "O pipeline de ASR pode ser decomposto em tr√™s est√°gios:\n",
    "\n",
    "1. Um extrator de caracter√≠sticas que pr√©-processa os √°udios brutos de entrada\n",
    "2. O modelo que realiza o mapeamento sequ√™ncia a sequ√™ncia\n",
    "3. Um tokenizador que p√≥s-processa as sa√≠das do modelo para o formato de texto\n",
    "\n",
    "No ü§ó Transformers, o modelo Whisper possui um extrator de caracter√≠sticas e um tokenizador associados, chamados respectivamente de [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor) e [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer).\n",
    "\n",
    "Vamos passar pelos detalhes para configurar o extrator de caracter√≠sticas e o tokenizador, um por vez!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560332eb-3558-41a1-b500-e83a9f695f84",
   "metadata": {
    "id": "560332eb-3558-41a1-b500-e83a9f695f84"
   },
   "source": [
    "### Carregando WhisperFeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec8068-0bd7-412d-b662-0edb9d1e7365",
   "metadata": {
    "id": "32ec8068-0bd7-412d-b662-0edb9d1e7365"
   },
   "source": [
    "O extrator de caracter√≠sticas do Whisper realiza duas opera√ß√µes:\n",
    "1. Preenche / trunca os √°udios de entrada para 30s: quaisquer √°udios de entrada mais curtos que 30s s√£o preenchidos com sil√™ncio (zeros) at√© 30s, e os que t√™m mais de 30s s√£o truncados para 30s.\n",
    "2. Converte os √°udios de entrada em caracter√≠sticas de entrada de _espectrograma log-Mel_, uma representa√ß√£o visual do √°udio e a forma de entrada esperada pelo modelo Whisper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d9ec1-d12b-4b64-93f7-04c63997da19",
   "metadata": {
    "id": "589d9ec1-d12b-4b64-93f7-04c63997da19"
   },
   "source": [
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/spectrogram.jpg\" alt=\"Trulli\" style=\"width:100%\">\n",
    "<figcaption align=\"center\"><b>Figura 2:</b> Convers√£o do array de √°udio amostrado para espectrograma log-Mel. \n",
    "Esquerda: sinal de √°udio unidimensional amostrado. Direita: espectrograma log-Mel correspondente. Fonte da figura: \n",
    "<a href=\"https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html\">Blog do Google SpecAugment</a>.\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef54d5-b946-4c1d-9fdc-adc5d01b46aa",
   "metadata": {
    "id": "b2ef54d5-b946-4c1d-9fdc-adc5d01b46aa"
   },
   "source": [
    "Carregaremos o extrator de recursos do ponto de verifica√ß√£o pr√©-treinado com os valores padr√£o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5",
   "metadata": {
    "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93748af7-b917-4ecf-a0c8-7d89077ff9cb",
   "metadata": {
    "id": "93748af7-b917-4ecf-a0c8-7d89077ff9cb"
   },
   "source": [
    "### Carregando o WhisperTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc82609-a9fb-447a-a2af-99597c864029",
   "metadata": {
    "id": "2bc82609-a9fb-447a-a2af-99597c864029"
   },
   "source": [
    "O modelo Whisper gera uma sequ√™ncia de _ids de tokens_. O tokenizador mapeia cada um desses ids de tokens para suas respectivas strings de texto. Para o Portugu√™s, podemos carregar o tokenizador pr√©-treinado e us√°-lo para fine-tuning sem nenhuma modifica√ß√£o adicional. Basta especificarmos o idioma de destino e a tarefa. Esses argumentos informam ao tokenizador para prefixar os tokens de idioma e tarefa no in√≠cio das sequ√™ncias de r√≥tulos codificados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "90d056e20b3e4f14ae0199a1a4ab1bb0",
      "d82a88daec0e4f14add691b7b903064c",
      "350acdb0f40e454099fa901e66de55f0",
      "2e6a82a462cc411d90fa1bea4ee60790",
      "c74bfee0198b4817832ea86e8e88d96c",
      "04fb2d81eff646068e10475a08ae42f4"
     ]
    },
    "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
    "outputId": "5c004b44-86e7-4e00-88be-39e0af5eed69"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Portuguese\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef23f3-f4a8-483a-a2dc-080a7496cb1b",
   "metadata": {
    "id": "d2ef23f3-f4a8-483a-a2dc-080a7496cb1b"
   },
   "source": [
    "### Combinando para criar um WhisperProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff67654-5a29-4bb8-a69d-0228946c6f8d",
   "metadata": {
    "id": "5ff67654-5a29-4bb8-a69d-0228946c6f8d"
   },
   "source": [
    "Para simplificar o uso do extrator de caracter√≠sticas e do tokenizador, podemos _agrupar_ ambos em uma √∫nica classe `WhisperProcessor`. Este objeto processador herda da `WhisperFeatureExtractor` e `WhisperTokenizer`, e pode ser usado nas entradas de √°udio e nas previs√µes do modelo conforme necess√°rio. Ao fazer isso, precisamos apenas acompanhar dois objetos durante o treinamento: o `processador` e o `modelo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6",
   "metadata": {
    "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Potuguese\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c",
   "metadata": {
    "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c"
   },
   "source": [
    "### Preparando os Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649bf01-2e8a-45e5-8fca-441c13637b8f",
   "metadata": {
    "id": "9649bf01-2e8a-45e5-8fca-441c13637b8f"
   },
   "source": [
    "Vamos imprimir o primeiro exemplo do conjunto de dados Common Voice para ver\n",
    "em que formato os dados est√£o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255",
   "metadata": {
    "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/home/lcad/.cache/huggingface/datasets/downloads/extracted/8e9e219409ef4bc4521f2b3145bee0e7851c99e7f4a6f4bcef5a61b75c81f339/pt_train_0/common_voice_pt_33954672.mp3', 'array': array([ 1.77635684e-15, -5.41788836e-14, -9.05941988e-14, ...,\n",
      "        1.72863140e-10,  1.45055787e-10,  7.10012743e-11]), 'sampling_rate': 48000}, 'sentence': 'Sinta-se feliz com a vit√≥ria que voc√™ ganha.', 'variant': 'Portuguese (Brasil)'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd",
   "metadata": {
    "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd"
   },
   "source": [
    "Como nosso √°udio de entrada √© amostrado a 48kHz, precisamos _reduzir a taxa de amostragem_ para 16kHz antes de pass√°-lo para o extrator de caracter√≠sticas do Whisper, sendo 16kHz a taxa de amostragem esperada pelo modelo Whisper.\n",
    "\n",
    "Vamos ajustar as entradas de √°udio para a taxa de amostragem correta usando o m√©todo [`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column) do dataset. Essa opera√ß√£o n√£o altera o √°udio no local, mas sim instrui o `datasets` a reamostrar as amostras de √°udio _on the fly_ na primeira vez que elas forem carregadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f12e2e57-156f-417b-8cfb-69221cc198e8",
   "metadata": {
    "id": "f12e2e57-156f-417b-8cfb-69221cc198e8"
   },
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707",
   "metadata": {
    "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707"
   },
   "source": [
    "Recarregar a primeira amostra de √°udio no conjunto de dados Common Voice ir√° reamostra-la para a taxa de amostragem desejada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87122d71-289a-466a-afcf-fa354b18946b",
   "metadata": {
    "id": "87122d71-289a-466a-afcf-fa354b18946b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/home/lcad/.cache/huggingface/datasets/downloads/extracted/8e9e219409ef4bc4521f2b3145bee0e7851c99e7f4a6f4bcef5a61b75c81f339/pt_train_0/common_voice_pt_33954672.mp3', 'array': array([-2.32830644e-09, -1.62981451e-09, -6.98491931e-10, ...,\n",
      "        1.58855151e-11,  8.93578544e-11,  1.82353688e-10]), 'sampling_rate': 16000}, 'sentence': 'Sinta-se feliz com a vit√≥ria que voc√™ ganha.', 'variant': 'Portuguese (Brasil)'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edc72d-08f8-4f01-899d-74e65ce441fc",
   "metadata": {
    "id": "91edc72d-08f8-4f01-899d-74e65ce441fc"
   },
   "source": [
    "Agora podemos escrever uma fun√ß√£o para preparar os dados prontos para o modelo:\n",
    "\n",
    "1. Carregamos e reamostramos os dados de √°udio chamando `batch[\"audio\"]`. Como explicado acima, o ü§ó Datasets realiza qualquer opera√ß√£o de reamostragem necess√°ria automaticamente.\n",
    "2. Usamos o extrator de caracter√≠sticas para calcular as caracter√≠sticas de entrada do espectrograma log-Mel a partir do nosso array de √°udio unidimensional.\n",
    "3. Codificamos as transcri√ß√µes para ids de r√≥tulos atrav√©s do uso do tokenizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6525c478-8962-4394-a1c4-103c54cce170",
   "metadata": {
    "id": "6525c478-8962-4394-a1c4-103c54cce170"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13",
   "metadata": {
    "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13"
   },
   "source": [
    "Podemos aplicar a fun√ß√£o de prepara√ß√£o dos dados a todos os nossos exemplos de treinamento usando o m√©todo `.map` do dataset. O argumento `num_proc` especifica quantos n√∫cleos de CPU usar. Definir `num_proc` > 1 ativar√° o processamento multiprocessado. Se o m√©todo `.map` travar com o processamento multiprocessado, defina `num_proc=1` e processe o dataset sequencialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
   "metadata": {
    "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b"
   },
   "outputs": [],
   "source": [
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)#num_proc=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a5a58-0239-4a25-b0df-c625fc9c5810",
   "metadata": {
    "id": "263a5a58-0239-4a25-b0df-c625fc9c5810"
   },
   "source": [
    "## Treinamento e Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693e768-c5a6-453f-89a1-b601dcf7daf7",
   "metadata": {
    "id": "a693e768-c5a6-453f-89a1-b601dcf7daf7"
   },
   "source": [
    "Agora que preparamos nossos dados, estamos prontos para mergulhar no pipeline de treinamento. O [ü§ó Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) far√° grande parte do trabalho pesado para n√≥s. Tudo o que precisamos fazer √©:\n",
    "\n",
    "- **Carregar um ponto de verifica√ß√£o pr√©-treinado**: precisamos carregar um ponto de verifica√ß√£o pr√©-treinado e configur√°-lo corretamente para o treinamento.\n",
    "\n",
    "- **Definir um collator de dados**: o collator de dados pega nossos dados pr√©-processados e os prepara em tensores PyTorch prontos para o modelo.\n",
    "\n",
    "- **M√©tricas de avalia√ß√£o**: durante a avalia√ß√£o, queremos avaliar o modelo usando a m√©trica [taxa de erro de palavras (WER)](https://huggingface.co/metrics/wer). Precisamos definir uma fun√ß√£o `compute_metrics` que fa√ßa esse c√°lculo.\n",
    "\n",
    "- **Definir a configura√ß√£o de treinamento**: essa configura√ß√£o ser√° usada pelo ü§ó Trainer para definir o cronograma de treinamento.\n",
    "\n",
    "Depois de fazer o fine-tuning do modelo, vamos avali√°-lo nos dados de teste para verificar se o treinamos corretamente para transcrever fala em Portugu√™s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2a825-6d9f-4a23-b145-c37c0039075b",
   "metadata": {
    "id": "daf2a825-6d9f-4a23-b145-c37c0039075b"
   },
   "source": [
    "###¬†Carregando um ponto de verifica√ß√£o pr√©-treinado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a97fa-4864-476b-8abc-f28b8166cfa5",
   "metadata": {
    "id": "437a97fa-4864-476b-8abc-f28b8166cfa5"
   },
   "source": [
    "Come√ßaremos nosso processo de fine-tuning a partir do ponto de verifica√ß√£o pr√©-treinado do Whisper `small`, cujos pesos precisamos carregar do Hugging Face Hub. Novamente, isso √© simples utilizando o ü§ó Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f",
   "metadata": {
    "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ead5f-2277-4a39-937b-585c2497b2df",
   "metadata": {
    "id": "a15ead5f-2277-4a39-937b-585c2497b2df"
   },
   "source": [
    "Podemos desabilitar a tarefa de detec√ß√£o autom√°tica de idioma realizada durante a infer√™ncia e for√ßar o modelo a gerar em Portugu√™s. Para fazer isso, definimos os argumentos [language](https://huggingface.co/docs/transformers/en/model_doc/whisper#transformers.WhisperForConditionalGeneration.generate.language) e [task](https://huggingface.co/docs/transformers/en/model_doc/whisper#transformers.WhisperForConditionalGeneration.generate.task) na configura√ß√£o de gera√ß√£o. Tamb√©m definiremos quaisquer [`forced_decoder_ids`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids) como None, j√° que esta era a maneira legada de definir os argumentos de idioma e tarefa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62038ba3-88ed-4fce-84db-338f50dcd04f",
   "metadata": {
    "id": "62038ba3-88ed-4fce-84db-338f50dcd04f"
   },
   "outputs": [],
   "source": [
    "model.generation_config.language = \"portuguese\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d230e6d-624c-400a-bbf5-fa660881df25",
   "metadata": {
    "id": "8d230e6d-624c-400a-bbf5-fa660881df25"
   },
   "source": [
    "### Define a Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04def221-0637-4a69-b242-d3f0c1d0ee78",
   "metadata": {
    "id": "04def221-0637-4a69-b242-d3f0c1d0ee78"
   },
   "source": [
    "O collator de dados para um modelo de fala sequ√™ncia-para-sequ√™ncia √© √∫nico no sentido de que ele trata os `input_features` e os `labels` de forma independente: os `input_features` devem ser manipulados pelo extrator de caracter√≠sticas e os `labels` pelo tokenizador.\n",
    "\n",
    "Os `input_features` j√° est√£o preenchidos para 30s e convertidos para um espectrograma log-Mel de dimens√£o fixa pela a√ß√£o do extrator de caracter√≠sticas, ent√£o tudo o que precisamos fazer √© converter os `input_features` para tensores PyTorch em lotes. Fazemos isso utilizando o m√©todo `.pad` do extrator de caracter√≠sticas com `return_tensors=pt`.\n",
    "\n",
    "Por outro lado, os `labels` n√£o est√£o preenchidos. Primeiro, preenchemos as sequ√™ncias at√© o comprimento m√°ximo no lote usando o m√©todo `.pad` do tokenizador. Os tokens de preenchimento s√£o ent√£o substitu√≠dos por `-100` para que esses tokens **n√£o** sejam levados em conta ao calcular a perda. Em seguida, cortamos o token BOS (Beginning of Sequence) do in√≠cio da sequ√™ncia de r√≥tulos, pois o adicionamos novamente mais tarde durante o treinamento.\n",
    "\n",
    "Podemos aproveitar o `WhisperProcessor` que definimos anteriormente para realizar tanto as opera√ß√µes do extrator de caracter√≠sticas quanto as do tokenizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5",
   "metadata": {
    "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86",
   "metadata": {
    "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86"
   },
   "source": [
    "Vamos inicializar o coletor de dados que acabamos de definir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc834702-c0d3-4a96-b101-7b87be32bf42",
   "metadata": {
    "id": "fc834702-c0d3-4a96-b101-7b87be32bf42"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698",
   "metadata": {
    "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698"
   },
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fee1a7-a44c-461e-b047-c3917221572e",
   "metadata": {
    "id": "66fee1a7-a44c-461e-b047-c3917221572e"
   },
   "source": [
    "Usaremos a m√©trica de taxa de erro de palavras (WER), a m√©trica 'de-facto' para avaliar sistemas ASR. Para mais informa√ß√µes, consulte a documenta√ß√£o do WER [aqui](https://huggingface.co/metrics/wer). Vamos carregar a m√©trica WER do ü§ó Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b22b4011-f31f-4b57-b684-c52332f92890",
   "metadata": {
    "id": "b22b4011-f31f-4b57-b684-c52332f92890"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f32cab6-31f0-4cb9-af4c-40ba0f5fc508",
   "metadata": {
    "id": "4f32cab6-31f0-4cb9-af4c-40ba0f5fc508"
   },
   "source": [
    "Ent√£o, precisamos simplesmente definir uma fun√ß√£o que receba as previs√µes do nosso modelo e retorne a m√©trica WER. Essa fun√ß√£o, chamada `compute_metrics`, primeiro substitui `-100` pelo `pad_token_id` nos `label_ids` (desfazendo o passo que aplicamos no collator de dados para ignorar corretamente os tokens de preenchimento na perda). Em seguida, ela decodifica os ids previstos e os ids dos r√≥tulos para strings. Finalmente, ela calcula o WER entre as previs√µes e os r√≥tulos de refer√™ncia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23959a70-22d0-4ffe-9fa1-72b61e75bb52",
   "metadata": {
    "id": "23959a70-22d0-4ffe-9fa1-72b61e75bb52"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06",
   "metadata": {
    "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06"
   },
   "source": [
    "### Define the Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21af1e9-0188-4134-ac82-defc7bdcc436",
   "metadata": {
    "id": "c21af1e9-0188-4134-ac82-defc7bdcc436"
   },
   "source": [
    "No passo final, definimos todos os par√¢metros relacionados ao treinamento. Para mais detalhes sobre os argumentos de treinamento, consulte a documenta√ß√£o do **Seq2SeqTrainingArguments** [aqui](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a",
   "metadata": {
    "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcad/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-pt-br\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16, # diminuir para 8 ou 4 se encontrar problemas de mem√≥ria\n",
    "    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size (Se voc√™ diminuir o per_device_train_batch_size, pode aumentar o n√∫mero de gradient_accumulation_steps para acumular gradientes e simular um maior batch size)\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=1000,\n",
    "    #max_steps=8000,\n",
    "    num_train_epochs = 2,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",  #\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    "    resume_from_checkpoint=r\"/home/lcad/Documents/whisper-small-pt-br/checkpoint-2000/\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a944d8-3112-4552-82a0-be25988b3857",
   "metadata": {
    "id": "b3a944d8-3112-4552-82a0-be25988b3857"
   },
   "source": [
    "**Nota**: se n√£o desejar enviar os pontos de verifica√ß√£o do modelo para o Hub, defina `push_to_hub=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac29114-d226-4f54-97cf-8718c9f94e1e",
   "metadata": {
    "id": "bac29114-d226-4f54-97cf-8718c9f94e1e"
   },
   "source": [
    "Podemos encaminhar os argumentos de treinamento para o ü§ó Trainer junto com nosso modelo, conjunto de dados, collator de dados e a fun√ß√£o `compute_metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d546d7fe-0543-479a-b708-2ebabec19493",
   "metadata": {
    "id": "d546d7fe-0543-479a-b708-2ebabec19493"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uOrRhDGtN5S4",
   "metadata": {
    "id": "uOrRhDGtN5S4"
   },
   "source": [
    "Vamos salvar o objeto do processador uma vez antes de iniciar o treinamento. Como o processador n√£o √© trein√°vel, ele n√£o mudar√° durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "-2zQwMfEOBJq",
   "metadata": {
    "id": "-2zQwMfEOBJq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f404cf9-4345-468c-8196-4bd101d9bd51",
   "metadata": {
    "id": "7f404cf9-4345-468c-8196-4bd101d9bd51"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b8d56-5a70-4f68-bd2e-f0752d0bd112",
   "metadata": {
    "id": "5e8b8d56-5a70-4f68-bd2e-f0752d0bd112"
   },
   "source": [
    "O treinamento levar√° aproximadamente de 5 a 10 horas, dependendo da sua GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a55168b-2f46-4678-afa0-ff22257ec06d",
   "metadata": {
    "id": "5a55168b-2f46-4678-afa0-ff22257ec06d"
   },
   "source": [
    "A mem√≥ria m√°xima da GPU para a configura√ß√£o de treinamento fornecida √© de aproximadamente 15,8 GB. Dependendo da GPU alocada para o Google Colab, √© poss√≠vel que voc√™ encontre um erro de CUDA `\"out-of-memory\"` ao iniciar o treinamento. \n",
    "\n",
    "Neste caso, voc√™ pode reduzir o `per_device_train_batch_size` progressivamente por fatores de 2 e utilizar o par√¢metro [`gradient_accumulation_steps`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.gradient_accumulation_steps) para compensar.\n",
    "\n",
    "Para iniciar o treinamento, basta executar:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796b817-857b-4476-b599-ef2cfb45e1b5",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache(): Libera mem√≥ria n√£o utilizada em PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f089ba8f-ca7a-48b5-b4bc-7d4737f5974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de",
   "metadata": {
    "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1964' max='1964' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1964/1964 63:49:17, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.183100</td>\n",
       "      <td>0.259262</td>\n",
       "      <td>17.379128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/lcad/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1964, training_loss=0.23888846345685893, metrics={'train_runtime': 229859.0685, 'train_samples_per_second': 0.273, 'train_steps_per_second': 0.009, 'total_flos': 1.813008918970368e+19, 'train_loss': 0.23888846345685893, 'epoch': 1.998473282442748})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ceeb56fd-3acc-46c8-8a20-a91b39bf87e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1184' max='1184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1184/1184 4:59:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.259261816740036, 'eval_wer': 17.37912830747441, 'eval_runtime': 18328.189, 'eval_samples_per_second': 0.517, 'eval_steps_per_second': 0.065, 'epoch': 1.998473282442748}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ced54-7187-4a06-b2fe-ba6dcca94dc3",
   "metadata": {
    "id": "810ced54-7187-4a06-b2fe-ba6dcca94dc3"
   },
   "source": [
    "Nosso melhor WER √© 32,0% - n√£o est√° mal para 8 horas de dados de treinamento! Podemos tornar nosso modelo mais acess√≠vel no Hub com tags e informa√ß√µes apropriadas no README.\n",
    "\n",
    "Voc√™ pode alterar esses valores para corresponder ao seu conjunto de dados, idioma e nome do modelo de acordo com suas necessidades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c704f91e-241b-48c9-b8e0-f0da396a9663",
   "metadata": {
    "id": "c704f91e-241b-48c9-b8e0-f0da396a9663"
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_17_0\",\n",
    "    \"dataset\": \"Common Voice 17.0\",  # a 'pretty' name for the training dataset\n",
    "    \"dataset_args\": \"config: pt, split: test\",\n",
    "    \"language\": \"pt\",\n",
    "    \"model_name\": \"Whisper Small Pt-Br - RFard\",  # \n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d676a-f944-4297-a938-a40eda0b2b68",
   "metadata": {
    "id": "090d676a-f944-4297-a938-a40eda0b2b68"
   },
   "source": [
    "Agora, os resultados do treinamento podem ser enviados para o Hub. Para fazer isso, execute o comando `push_to_hub` e salve o objeto preprocessor que criamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7030622-caf7-4039-939b-6195cdaa2585",
   "metadata": {
    "id": "d7030622-caf7-4039-939b-6195cdaa2585"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6426f52e-452a-44bb-9da1-e4241f1a0d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2025-03-25T11:40:41Z \u001b[0m\u001b[33mWARN \u001b[0m rustboard_core::run\u001b[0m\u001b[38;5;8m]\u001b[0m Read error in ./whisper-small-pt-br/runs/Mar21_00-44-41_lcad-XPS-8940/events.out.tfevents.1742528682.lcad-XPS-8940.7657.0: ReadRecordError(BadLengthCrc(ChecksumError { got: MaskedCrc(0x07980329), want: MaskedCrc(0x00000000) }))\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.19.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./whisper-small-pt-br/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4360d-5721-426e-b6ac-178f833fedeb",
   "metadata": {
    "id": "34d4360d-5721-426e-b6ac-178f833fedeb"
   },
   "source": [
    "## Building a Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65489b7-18d1-447c-ba69-cd28dd80dad9",
   "metadata": {
    "id": "e65489b7-18d1-447c-ba69-cd28dd80dad9"
   },
   "source": [
    "Agora que ajustamos nosso modelo, podemos criar uma demonstra√ß√£o para mostrar suas capacidades de ASR (Reconhecimento Autom√°tico de Fala)! Vamos utilizar o `pipeline` do ü§ó Transformers, que cuidar√° de todo o processo de ASR, desde o pr√©-processamento das entradas de √°udio at√© a decodifica√ß√£o das previs√µes do modelo.\n",
    "\n",
    "Executando o exemplo abaixo, ser√° gerada uma demonstra√ß√£o no Gradio onde podemos gravar fala atrav√©s do microfone do nosso computador e inseri-la no nosso modelo Whisper ajustado para transcrever o texto correspondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ace3aa-1ef3-45cb-933f-6ddca037c5aa",
   "metadata": {
    "id": "e0ace3aa-1ef3-45cb-933f-6ddca037c5aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcad/.local/lib/python3.8/site-packages/pydub/utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/processing_utils.py\", line 536, in audio_from_file\n",
      "    audio = AudioSegment.from_file(filename)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/pydub/audio_segment.py\", line 728, in from_file\n",
      "    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/pydub/utils.py\", line 274, in mediainfo_json\n",
      "    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n",
      "  File \"/usr/lib/python3.8/subprocess.py\", line 858, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/usr/lib/python3.8/subprocess.py\", line 1704, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'ffprobe'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1931, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1662, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs_cached))\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/components/audio.py\", line 231, in preprocess\n",
      "    sample_rate, data = processing_utils.audio_from_file(temp_file_path)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/processing_utils.py\", line 546, in audio_from_file\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Cannot load audio from file: `ffprobe` not found. Please install `ffmpeg` in your system to use non-WAV audio file formats and make sure `ffprobe` is in your PATH.\n",
      "/home/lcad/.local/lib/python3.8/site-packages/pydub/utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/processing_utils.py\", line 536, in audio_from_file\n",
      "    audio = AudioSegment.from_file(filename)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/pydub/audio_segment.py\", line 728, in from_file\n",
      "    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/pydub/utils.py\", line 274, in mediainfo_json\n",
      "    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n",
      "  File \"/usr/lib/python3.8/subprocess.py\", line 858, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/usr/lib/python3.8/subprocess.py\", line 1704, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'ffprobe'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1931, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1662, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs_cached))\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/components/audio.py\", line 231, in preprocess\n",
      "    sample_rate, data = processing_utils.audio_from_file(temp_file_path)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/processing_utils.py\", line 546, in audio_from_file\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Cannot load audio from file: `ffprobe` not found. Please install `ffmpeg` in your system to use non-WAV audio file formats and make sure `ffprobe` is in your PATH.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 2364, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 864, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_88932/493924419.py\", line 6, in transcribe\n",
      "    text = pipe(audio)[\"text\"]\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 283, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 1294, in __call__\n",
      "    return next(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 269, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 673, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 186, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 412, in preprocess\n",
      "    raise TypeError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "TypeError: We expect a numpy ndarray as input, got `<class 'NoneType'>`\n",
      "/home/lcad/.local/lib/python3.8/site-packages/pydub/utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/processing_utils.py\", line 536, in audio_from_file\n",
      "    audio = AudioSegment.from_file(filename)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/pydub/audio_segment.py\", line 728, in from_file\n",
      "    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/pydub/utils.py\", line 274, in mediainfo_json\n",
      "    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n",
      "  File \"/usr/lib/python3.8/subprocess.py\", line 858, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/usr/lib/python3.8/subprocess.py\", line 1704, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'ffprobe'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1931, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1662, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs_cached))\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/components/audio.py\", line 231, in preprocess\n",
      "    sample_rate, data = processing_utils.audio_from_file(temp_file_path)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/processing_utils.py\", line 546, in audio_from_file\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Cannot load audio from file: `ffprobe` not found. Please install `ffmpeg` in your system to use non-WAV audio file formats and make sure `ffprobe` is in your PATH.\n",
      "/home/lcad/.local/lib/python3.8/site-packages/pydub/utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/processing_utils.py\", line 536, in audio_from_file\n",
      "    audio = AudioSegment.from_file(filename)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/pydub/audio_segment.py\", line 728, in from_file\n",
      "    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/pydub/utils.py\", line 274, in mediainfo_json\n",
      "    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n",
      "  File \"/usr/lib/python3.8/subprocess.py\", line 858, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/usr/lib/python3.8/subprocess.py\", line 1704, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'ffprobe'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1931, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1662, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs_cached))\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/components/audio.py\", line 231, in preprocess\n",
      "    sample_rate, data = processing_utils.audio_from_file(temp_file_path)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/processing_utils.py\", line 546, in audio_from_file\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Cannot load audio from file: `ffprobe` not found. Please install `ffmpeg` in your system to use non-WAV audio file formats and make sure `ffprobe` is in your PATH.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 2364, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 864, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_88932/493924419.py\", line 6, in transcribe\n",
      "    text = pipe(audio)[\"text\"]\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 283, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 1294, in __call__\n",
      "    return next(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 269, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 673, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 186, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 412, in preprocess\n",
      "    raise TypeError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "TypeError: We expect a numpy ndarray as input, got `<class 'NoneType'>`\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 2364, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 864, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_88932/493924419.py\", line 6, in transcribe\n",
      "    text = pipe(audio)[\"text\"]\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 283, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 1294, in __call__\n",
      "    return next(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 269, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 673, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 186, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 412, in preprocess\n",
      "    raise TypeError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "TypeError: We expect a numpy ndarray as input, got `<class 'NoneType'>`\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/audio_utils.py\", line 34, in ffmpeg_read\n",
      "    with subprocess.Popen(ffmpeg_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE) as ffmpeg_process:\n",
      "  File \"/usr/lib/python3.8/subprocess.py\", line 858, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/usr/lib/python3.8/subprocess.py\", line 1704, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'ffmpeg'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 2364, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 864, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_88932/493924419.py\", line 6, in transcribe\n",
      "    text = pipe(audio)[\"text\"]\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 283, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 1294, in __call__\n",
      "    return next(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 269, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 673, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 186, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 364, in preprocess\n",
      "    inputs = ffmpeg_read(inputs, self.feature_extractor.sampling_rate)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/audio_utils.py\", line 37, in ffmpeg_read\n",
      "    raise ValueError(\"ffmpeg was not found but is required to load audio files from filename\") from error\n",
      "ValueError: ffmpeg was not found but is required to load audio files from filename\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 2364, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 864, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_88932/493924419.py\", line 6, in transcribe\n",
      "    text = pipe(audio)[\"text\"]\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 283, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 1294, in __call__\n",
      "    return next(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 269, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 673, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py\", line 186, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 412, in preprocess\n",
      "    raise TypeError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "TypeError: We expect a numpy ndarray as input, got `<class 'NoneType'>`\n",
      "/home/lcad/.local/lib/python3.8/site-packages/pydub/utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/processing_utils.py\", line 536, in audio_from_file\n",
      "    audio = AudioSegment.from_file(filename)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/pydub/audio_segment.py\", line 728, in from_file\n",
      "    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/pydub/utils.py\", line 274, in mediainfo_json\n",
      "    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n",
      "  File \"/usr/lib/python3.8/subprocess.py\", line 858, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/usr/lib/python3.8/subprocess.py\", line 1704, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'ffprobe'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1931, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/blocks.py\", line 1662, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs_cached))\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/components/audio.py\", line 231, in preprocess\n",
      "    sample_rate, data = processing_utils.audio_from_file(temp_file_path)\n",
      "  File \"/home/lcad/.local/lib/python3.8/site-packages/gradio/processing_utils.py\", line 546, in audio_from_file\n",
      "    raise RuntimeError(msg) from e\n",
      "RuntimeError: Cannot load audio from file: `ffprobe` not found. Please install `ffmpeg` in your system to use non-WAV audio file formats and make sure `ffprobe` is in your PATH.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(model=\"RodrigoFardin/whisper-small-pt-br\") \n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=gr.Audio(type=\"filepath\"),  # Remover 'source' aqui\n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Small Portuguese\",\n",
    "    description=\"Realtime demo for Portugues-Brazil speech recognition using a fine-tuned Whisper small model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca743fbd-602c-48d4-ba8d-a2fe60af64ba",
   "metadata": {
    "id": "ca743fbd-602c-48d4-ba8d-a2fe60af64ba"
   },
   "source": [
    "## Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f737783-2870-4e35-aa11-86a42d7d997a",
   "metadata": {
    "id": "7f737783-2870-4e35-aa11-86a42d7d997a"
   },
   "source": [
    "Neste blog, cobrimos um guia passo a passo sobre como ajustar o Whisper para ASR multil√≠ngue usando ü§ó Datasets, Transformers e o Hugging Face Hub. Para mais detalhes sobre o modelo Whisper, o conjunto de dados Common Voice e a teoria por tr√°s do ajuste fino, consulte o [post do blog](https://huggingface.co/blog/fine-tune-whisper). Se voc√™ tiver interesse em ajustar outros modelos Transformers, tanto para ASR em ingl√™s quanto multil√≠ngue, n√£o deixe de conferir os scripts de exemplo em [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
